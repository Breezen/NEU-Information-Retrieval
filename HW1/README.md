# Run crawling (with Python 3)
- `python WikiCrawler.py`
- follow the prompted information, enter the seed URL and keywords

# Running results
- Task 1 BFS: maximum depth 3
- Task 1 DFS: maximum depth 6
- Task 2 BFS: maximum depth 4

# Download raw htmls
- `python downloadHTML.py`
- follow the prompted information, enter the text file containing wiki links
- output htmls in ./htmls